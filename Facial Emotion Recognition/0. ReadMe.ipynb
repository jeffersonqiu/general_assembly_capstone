{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "403f6e84",
   "metadata": {},
   "source": [
    "# Facial Emotion Recognition\n",
    "## Executive Summary\n",
    "Significant amount of information is transmitted during human interraction through the use of facial expressions. With the increase usage of human-machine or human-machine-human interraction, this means of communication may not be transmitted efficiently due to the lack of emotion comprehension on machines. Therefore, there is a growing needs to improve upon this, to build a machine learning model that is able to identify human emotion so that it can react accordingly or inform another human. \n",
    "\n",
    "This type of problem is a suitable implementation for Convolutional Neural Network. This work managed to develop some models based on well-established pre-trained model and custom built. The final result shows that the best pre-trained model based on validation set accuracy is VGGFace Model with 75.5% accuracy while the custom built model achieved 66.7% of accuracy. The final model is trained on >6000 pictures consisting of 3 class of emotions, angry, sad and happy. \n",
    "\n",
    "The model finally is tested on actual footage in the form of photo and videos. It was able to predict fairly accurately based on changes in facial landmarks such as mouth shape, eyebrows shape and head position.\n",
    "\n",
    "## Background\n",
    "Emotional cues plays a crucial role in human communication. Some studies such as the one lead by [Dr Albert Meharabian](https://archive.org/details/silentmessagesim00mehr) noted that facial expression may have up to 55% of the impact to convey the message to the receiver. This importance is the reason why we prefer to talk face to face when discussion/ deciding an important matters even in the age of internet where call are cheap or even free. Interestingly, there are findings suggesting that human ability to 'detect' emotion varies across age and gender. For example, it is reported ([source1](https://academic.oup.com/psychsocgerontology/article/64B/6/696/552239?login=true), [source2](https://psycnet.apa.org/record/2007-03358-016)) in at least two studies that older adults are less perceptive of fear and anger in other people. It was also reported that ([source](https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02371/full)) female at all age are a better emotion intepreter than their male counterparts. \n",
    "\n",
    "This introduces an issue at the digitalization era where human-computer interraction is more prevalent than ever. Robot can understand what we say, but that only captures 7% impact of what we actually mean, again quoting the famous study by [Dr Albert Meharabian](https://archive.org/details/silentmessagesim00mehr). Therefore, it is natural that the interest of Facial Emotion Recognition has recently spiked. One estimate suggests, the emotion recognition industry could worth to $37 billion by 2026 ([source](https://www.nature.com/articles/d41586-021-00868-5)). One of the use case, namely 4 Little Trees monitor children's emotion while they do classwork. It identifies students' facial features such as happy, sad, angry, disgust, surprise or fear to gauge their motivation and forecast grades ([source](https://www.4littletrees.com/)). Some other applications for facial emotion recognition are:\n",
    "- Personal well-being monitoring system\n",
    "- Customer Service performance monitoring\n",
    "- Market research\n",
    "- Video game testing\n",
    "\n",
    "## Problem Statement\n",
    "Motivated by the myriad application mentioned above, the aim of this study is to **build a model to identify emotion based on facial expression**. \n",
    "\n",
    "## Proposed Approach\n",
    "Human naturally learns to recognize emotion based on the **facial landmarks** of the adults around us. This makes recognizing emotion a second nature to us. However, this is not as straightforward for computer as there are so many types of faces, skin tone and expression that prevents a hard-coded machine to perform as well as human. \n",
    "\n",
    "This opens up an opportunity for machine learning, specifically **Convolution Neural Network (CNN)** to enable machine to 'understand' facial emotion. The model will 'learn' the pattern of human emotion based on a collection of facial pictures labelled based on their corresponding emotion. Once the model has been trained, it will be deployed to some pictures and videos for testing. \n",
    "\n",
    "Overall, the **flow of work** is as follows:\n",
    "1. Find [facial emotion dataset](https://www.kaggle.com/mahmoudima/mma-facial-expression) (The dataset is available on Kaggle and can be stored under 'dataset' folder to follow the notebook along)\n",
    "2. Explore the dataset\n",
    "3. Select existing (pretrained) model(s) to perform emotion recognition on the dataset and tune it\n",
    "4. Build a CNN model and tune it\n",
    "5. Compare models to obtain the best model\n",
    "6. Try out the best model againts pictures and videos\n",
    "\n",
    "## Summary of Model Training\n",
    "\n",
    "Summary of facial expression detection models are as follows:\n",
    "\n",
    "\n",
    "|  No. |   Model Name   |     Type     | Best Validation Accuracy | Remarks                                                     |\n",
    "|:----:|:--------------:|:------------:|:------------------------:|-------------------------------------------------------------|\n",
    "|  0.  |    Baseline    |      -       |           33.3%          | Select all data to be majority class                        |\n",
    "|  1.  |      VGG16     |  Pre-trained |           63.5%          | Pretrained model                                            |\n",
    "|  2.  |     VGGFace    |  Pre-trained |           75.5%          | VGG16 trained on facial dataset                             |\n",
    "|  3.  |    ResNet50    |  Pre-trained |            50%           |                                                             |\n",
    "|  4.  | Custom Model 1 | Custom Built |           34.9%          | Ended up categorizing all data into happy faces                        |\n",
    "|  5.  | Custom Model 2 | Custom Built |           65.6%          | CNN with batch normalization                                |\n",
    "|  6.  | Custom Model 3 | Custom Built |           66.7%          | Find best combination of hyperparameter using keras tuner.  |\n",
    "\n",
    "**Pre-trained Models**\n",
    "\n",
    "The first three model (excluding the baseline model) is trained based on existing well-established model that is adapted for our use. The improvement is made by retraining the last block for every convolutional network for a few epochs until validation accuracy stops improving for 6 epochs in a row. As predicted, VGGFace outperforms all the model as it was pre-trained on millions of facial pictures (VGG Face Dataset). \n",
    "\n",
    "**Custom Built Models**\n",
    "\n",
    "Subsequently, we also developed a custom built model with 3 convolution blocks (each block consisting of 2 convolution layers), followed by 2 block of fully-connected network and 1 prediction layer. Interesting to note that initially the model (Custom Model 1) performs very badly (only performs as well as the baseline model) due to exploding gradient problem. The next iteration of the model (Custom Model 2) possess exactly the same network but with addition of Batch Normalization layer in between of the dense network. This model performs significantly better. Custom Model 3 inherits structure from the previous two models but with number of filter, dense unit and dropout decided by keras tuning random search. It improves the validation accuracy marginally. \n",
    "\n",
    "**Visualization of Models**\n",
    "\n",
    "Custom Model 2 was dissected to understand how it manages to extract facial landmarks to identify emotion from facial expression. Some observations:\n",
    "- Earlier layer extract outlines of the image. Some pictures still retain some degree of color of the image.\n",
    "- At the middle layers, pictures have been transformed into facial landmarks such as mouth, nose, eyes, etc.\n",
    "- Finally, at the last layers, picture has been totally reconstructed beyond human comprehension. This output is used as the training material for the downstream neural networks.\n",
    "\n",
    "**Model Performance on Real Footage**\n",
    "\n",
    "The best model (VGG Face) is tested to identify real footages. The model is able to detect facial expression based on different shape in mouth, eyebrow and head positioning. Functions to detect photo and videos are also written for trial.\n",
    "\n",
    "## Model Limitation and Recommendations\n",
    "Although the current model has performed fairly well under new footages, it still has some major limitations:\n",
    "\n",
    "1. Currently, the model is limited to identifying three emotions, angry, sad and happy. However, there are many other emotions such as disgust, fear, neutral, etc. that human naturally shows on daily basis. On top of these basic emotions, there are much more complex emotions such as confused, embarrassed, etc. Building a model to capture these expressions are much more useful for deployment, i.e. student monitoring or customer service quality monitoring system. \n",
    "\n",
    "2. The current models are trained with low quality images with many questionable labelling, some even does not contain a face! With a cleaner and better resolution pictures, the model can have a better accuracy score on a new facial footage. \n",
    "\n",
    "3. With the increasing mask usage during the pandemic, it will be useful to train a model based on masked images. This can either be done by collecting actual mask pictures or by covering facial features using a digital mask [artificially](https://www.pyimagesearch.com/2020/05/04/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
